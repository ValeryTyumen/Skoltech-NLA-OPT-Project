{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import artm\n",
    "from base_regularizer import BaseRegularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_word_in_doc_freqs(words_count, docs_count):\n",
    "\n",
    "    density = 0.01\n",
    "    max_freq = 5\n",
    "\n",
    "    word_in_doc_freqs = sparse.dok_matrix((words_count, docs_count), dtype=int)\n",
    "\n",
    "    for i in range(int(density*words_count*docs_count)):\n",
    "\n",
    "        word_index = np.random.choice(words_count)\n",
    "        doc_index = np.random.choice(docs_count)\n",
    "\n",
    "        word_in_doc_freqs[word_index, doc_index] = np.random.choice(max_freq) + 1\n",
    "\n",
    "    return word_in_doc_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroRegularizer(BaseRegularizer):\n",
    "\n",
    "    def __init__(self, words_count, docs_count, topics_count):\n",
    "\n",
    "        self._word_in_topics_probs_grad = np.zeros((words_count, topics_count))\n",
    "        self._topic_in_doc_probs_grad = np.zeros((topics_count, docs_count))\n",
    "\n",
    "    def get_value(self, word_in_topics_probs, topic_in_doc_probs):\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    def get_gradient(self, word_in_topics_probs, topic_in_doc_probs):\n",
    "\n",
    "        return self._word_in_topics_probs_grad, self._topic_in_doc_probs_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=0)\n",
    "\n",
    "words_count = 10000\n",
    "docs_count = 100\n",
    "topics_count = 10\n",
    "\n",
    "word_in_doc_freqs = generate_word_in_doc_freqs(words_count, docs_count)\n",
    "words_list = np.array([str(i) for i in range(words_count)])\n",
    "\n",
    "zero_regularizer = ZeroRegularizer(words_count, docs_count, topics_count)\n",
    "\n",
    "artm_model = artm.ARTM(topics_count, [zero_regularizer], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter#1: loglike=-253375.08594793288\n",
      "iter#2: loglike=-248155.68445356327\n",
      "iter#3: loglike=-240993.1539559371\n",
      "iter#4: loglike=-232894.04538079945\n",
      "iter#5: loglike=-225252.51292302832\n",
      "iter#6: loglike=-218928.35373559463\n",
      "iter#7: loglike=-214095.58940937347\n",
      "iter#8: loglike=-210671.68357721512\n",
      "iter#9: loglike=-208376.48245071087\n",
      "iter#10: loglike=-206869.89796774142\n"
     ]
    }
   ],
   "source": [
    "train_result = artm_model.train(word_in_doc_freqs, words_list, iterations_count=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['7326', '5398', '9117', '2136', '3057', '5825', '1747', '3845',\n",
       "        '1692', '7526'],\n",
       "       ['2824', '6332', '7097', '2735', '2789', '9496', '4471', '5995',\n",
       "        '902', '3957'],\n",
       "       ['9537', '7037', '2874', '9457', '1695', '2953', '3248', '367',\n",
       "        '3392', '6169'],\n",
       "       ['1632', '7464', '3708', '2460', '8019', '2522', '682', '8904',\n",
       "        '1609', '6327'],\n",
       "       ['3419', '1221', '6538', '5146', '8160', '4479', '621', '6762',\n",
       "        '9477', '2168'],\n",
       "       ['1100', '1273', '6923', '9966', '7853', '999', '4459', '3713',\n",
       "        '6223', '5479'],\n",
       "       ['4740', '3764', '1372', '5311', '5607', '4980', '7415', '1671',\n",
       "        '6583', '692'],\n",
       "       ['9098', '2755', '3996', '9536', '2010', '1564', '1265', '5865',\n",
       "        '4267', '3825'],\n",
       "       ['7932', '4186', '1004', '8174', '3555', '0', '8986', '6514',\n",
       "        '1983', '2003'],\n",
       "       ['2317', '1565', '9275', '8777', '5807', '6572', '2547', '5213',\n",
       "        '7543', '4420']],\n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result.get_top_words_in_topics(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import artm\n",
    "from base_regularizer import BaseRegularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_word_in_doc_freqs(words_count, docs_count):\n",
    "\n",
    "    density = 0.01\n",
    "    max_freq = 5\n",
    "\n",
    "    word_in_doc_freqs = sparse.dok_matrix((words_count, docs_count), dtype=int)\n",
    "\n",
    "    for i in range(int(density*words_count*docs_count)):\n",
    "\n",
    "        word_index = np.random.choice(words_count)\n",
    "        doc_index = np.random.choice(docs_count)\n",
    "\n",
    "        word_in_doc_freqs[word_index, doc_index] = np.random.choice(max_freq) + 1\n",
    "\n",
    "    return word_in_doc_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroRegularizer(BaseRegularizer):\n",
    "\n",
    "    def __init__(self, words_count, docs_count, topics_count):\n",
    "\n",
    "        self._word_in_topics_probs_grad = np.zeros((words_count, topics_count))\n",
    "        self._topic_in_doc_probs_grad = np.zeros((topics_count, docs_count))\n",
    "\n",
    "    def get_value(self, word_in_topics_probs, topic_in_doc_probs):\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    def get_gradient(self, word_in_topics_probs, topic_in_doc_probs):\n",
    "\n",
    "        return self._word_in_topics_probs_grad, self._topic_in_doc_probs_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = 10000\n",
    "docs_count = 100\n",
    "topics_count = 10\n",
    "\n",
    "word_in_doc_freqs = generate_word_in_doc_freqs(words_count, docs_count)\n",
    "words_list = np.array([str(i) for i in range(words_count)])\n",
    "\n",
    "zero_regularizer = ZeroRegularizer(words_count, docs_count, topics_count)\n",
    "\n",
    "artm_model = artm.ARTM(topics_count, [zero_regularizer], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter#1: loglike=-249709.4873482039\n",
      "iter#2: loglike=-244631.21241853482\n",
      "iter#3: loglike=-237713.0817303567\n",
      "iter#4: loglike=-229910.31317012556\n",
      "iter#5: loglike=-222479.7952583897\n",
      "iter#6: loglike=-216177.54461735833\n",
      "iter#7: loglike=-211287.45543143657\n",
      "iter#8: loglike=-207815.25438644484\n",
      "iter#9: loglike=-205532.86666285899\n",
      "iter#10: loglike=-203990.8659191058\n"
     ]
    }
   ],
   "source": [
    "train_result = artm_model.train(word_in_doc_freqs, words_list, iterations_count=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['443', '4689', '5924', '1295', '676', '7474', '6804', '2371',\n",
       "        '5695', '3814'],\n",
       "       ['4970', '4755', '6780', '4466', '5060', '501', '837', '8655',\n",
       "        '8552', '8402'],\n",
       "       ['7105', '4697', '6634', '8244', '8366', '4140', '6420', '4094',\n",
       "        '7929', '3577'],\n",
       "       ['3898', '7945', '7753', '8269', '4224', '1916', '9959', '1292',\n",
       "        '5497', '8358'],\n",
       "       ['7990', '8561', '5329', '1918', '1858', '8805', '7795', '9386',\n",
       "        '7221', '797'],\n",
       "       ['8229', '6019', '7219', '5716', '1443', '2189', '417', '1229',\n",
       "        '6380', '1341'],\n",
       "       ['8567', '1824', '7602', '6167', '4771', '7248', '8520', '1303',\n",
       "        '5801', '3179'],\n",
       "       ['1001', '7821', '2879', '5426', '4686', '5163', '906', '77',\n",
       "        '3742', '5969'],\n",
       "       ['9224', '7699', '52', '6909', '4923', '2640', '3567', '1566',\n",
       "        '8916', '5631'],\n",
       "       ['2005', '9542', '7009', '7739', '3994', '5365', '6430', '750',\n",
       "        '8941', '9067']],\n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result.get_top_words_in_topics(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
